Question 110- Create a Delta Table with Schema Enforcement: Ensure data integrity by enforcing a predefined schema on Delta tables during data writes? 
Step1- create table
  # Create delta table
  from delta.tables import *
  # Create Delta table
  DeltaTable.create(spark) \
      .tableName("emp_schema9") \
      .addColumn("ID", "INT") \
      .addColumn("Name", "STRING") \
      .location("/FileStore/Tables/Delta/emp_schema9") \
      .execute()
step2- insert some record and print it
  %sql
  insert into emp_schema9 values(1, 'Gaurav');
step3- Schema Evolution
  # let us assume we receive one .csv file which is having one extra column
  
  from pyspark.sql.types import StructType, StructField, StringType, IntegerType
  
  # Sample data
  data = [
      (1, "John","test_data")
  ]
  
  # Define schema for DataFrame
  schema = StructType([
      StructField("ID", IntegerType(), True),
      StructField("Name", StringType(), True),
      StructField("additional_column1", StringType(), True)
  ])
  
  # Create DataFrame
  df = spark.createDataFrame(data, schema)
  
  # Show DataFrame
  display(df)
Step4- If we combine this two tables then it would get failed as second file we have extra column
  df.write.format('delta').mode('append').saveAsTable('emp_schema9')
step5- 
  Use df.write.option('mergeSchema', 'true') to handle the schema mismatch
  df.write.option('mergeSchema','true').format('delta').mode('append').saveAsTable('emp_schema9')
=========================================================================================================================================================
Question 111-Create a Partitioned Delta Table: Organize data into logical subsets based on specified columns for improved query performance?
%sql
-- Create the Partitioned Delta Table
CREATE TABLE partitioned_emp_sql(
  ID INT,
  Name STRING,
  Country STRING
)
USING DELTA
--partition the data
LOCATION '/FileStore/Tables/Delta/partitioned_emp_sql';

-- Insert Data into the Partitioned Delta Table
INSERT INTO partitioned_emp_sql VALUES
(1, 'Alice', 'India'),
(2, 'Bob', 'USA'),
(3, 'Charlie', 'France'),
(4, 'David', 'Germany'),
(5, 'Eva', 'Japan');
=========================================================================================================================================================
Question 112- Read Specific Data Subsets using Partition Filters: Efficiently query and retrieve data by filtering on partition columns to reduce data scanning?
=========================================================================================================================================================
Question 113- Read Old Table Versions with Time Travel Filters: Access historical table versions by specifying a timestamp or version number for analysis or rollback?
=========================================================================================================================================================
Question 114- Perform Data Merges (Updates): Update, insert, or delete data based on specified conditions to synchronize or incrementally update the table?
=========================================================================================================================================================
Question 115- Analyze Delta Table History: Examine the transaction log to track changes, understand table evolution, and diagnose issues?
=========================================================================================================================================================
Question 116- Rollback to a Previous Table Version: Revert the Delta table to a previous state by specifying a timestamp or version number for rollback?
=========================================================================================================================================================
Question 117- How to filter specific row of delta table history on condition based (deltatable.history().filter(version==1)?
=========================================================================================================================================================
Question 118- Create Two Notebooks Notebook1 and Notebook2 , execute or call Notebook1 inside NoteBook2?
=========================================================================================================================================================
Question 119- Create two NoteBooks one Note Book Create Employee Dataframe,Second NoteBook Create Dept dataframe, in Second Note Join Dept daaframe with Emp  Dataframe?

# Step1- create empdf in first notebook

# Create the Employee DataFrame
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Define schema for Employee DataFrame
emp_schema = StructType([
    StructField("emp_id", IntegerType(), True),
    StructField("emp_name", StringType(), True),
    StructField("dept_id", IntegerType(), True)
])

# Sample employee data
emp_data = [
    (1, "Alice", 101),
    (2, "Bob", 102),
    (3, "Charlie", 101),
    (4, "David", 103)
]

# Create Employee DataFrame
emp_df = spark.createDataFrame(data=emp_data, schema=emp_schema)

# Display the Employee DataFrame
emp_df.show()
------------------------------------------
# Step 2- Create deptdf in second notebook

# Create the Department DataFrame

from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Define schema for Department DataFrame
dept_schema = StructType([
    StructField("dept_id", IntegerType(), True),
    StructField("dept_name", StringType(), True)
])

# Sample department data
dept_data = [
    (101, "HR"),
    (102, "Engineering"),
    (103, "Marketing")
]

# Create Department DataFrame
dept_df = spark.createDataFrame(data=dept_data, schema=dept_schema)

# Display the Department DataFrame
dept_df.show()
----------------------------------------------

=========================================================================================================================================================
Question 120- How to pass result of one Notebook into another Notebook (use result of notebook1 into notebook2)?
=========================================================================================================================================================
Question 121- How to Create Parameters and pass as input to Notebook?
=========================================================================================================================================================
Question 122- Create dataframe contains 10 rows, i want you to write this data into multiple files like parquet formate. pass parameter value. 3?
=========================================================================================================================================================
Question 123- How to do the error handling in Databricks notebooks. Is it possible to drop an email and save that log information into a file saved in a Blob?
=========================================================================================================================================================
Question 124- How to schedule notebooks without using Data Factory or Databricks jobs?
=========================================================================================================================================================
Question 125- I want to run the notebooks without using data factory or databricks?
# By using python scripts
import requests
import json

# Databricks Configuration
DATABRICKS_INSTANCE = "https://<your-databricks-instance>"  # e.g., https://adb-1234567890123456.5.azuredatabricks.net
TOKEN = "your-personal-access-token"
NOTEBOOK_PATH = "/path/to/notebook"  # Full path to the Databricks notebook
CLUSTER_ID = "your-cluster-id"      # Cluster ID to run the notebook

# REST API URL
URL = f"{DATABRICKS_INSTANCE}/api/2.1/jobs/runs/submit"

# Payload to trigger the notebook
payload = {
    "run_name": "Scheduled Notebook Run",
    "existing_cluster_id": CLUSTER_ID,
    "notebook_task": {
        "notebook_path": NOTEBOOK_PATH
    }
}

# Make the API request
headers = {"Authorization": f"Bearer {TOKEN}"}
response = requests.post(URL, headers=headers, json=payload)

# Check the response
if response.status_code == 200:
    print("Notebook run triggered successfully!")
    print("Response:", json.dumps(response.json(), indent=2))
else:
    print("Failed to trigger the notebook!")
    print("Response:", response.text)

=========================================================================================================================================================
Question 126- I want to run Datafactory pipelines without using triggers in datafactory?
=========================================================================================================================================================
Question 127- How to monitor and see the log analytics of Jobs scheduled for the Data factory pipeline and Databricks notebooks?
=========================================================================================================================================================
Question 128- How to maintain the version control of the notebooks with Azure Devops Git repository?
=========================================================================================================================================================








Question 129- What are the techniques used to improve the performance of a Notebook? - broadcast variable , broadcast joins, caching etc...

