
------------Unity Catalog----------------------------------
5. Create table under schema and add some records
%sql
CREATE TABLE bronze.emp(
  ID INT, NAME STRING, DEPT STRING
);

--Add some records
INSERT INTO main.bronze.emp VALUES
(1, 'John', 'Sales'),
(2, 'Alice', 'HR'),
(3, 'Bob', 'IT');

-- View the records
%sql
SELECT * FROM main.bronze.emp;

8. if you want to create a table in the legacy hive_metastore
%sql
--1. Use the hive_metastore catalog and a schema (e.g., default or your own):
USE CATALOG hive_metastore;
USE SCHEMA default;

-- 2. Create the table:
CREATE TABLE IF NOT EXISTS emp (
  ID INT,
  NAME STRING,
  DEPT STRING
);

10- create external location
%sql
-- Use legacy metastore instead of Unity Catalog
USE CATALOG hive_metastore;
USE SCHEMA default;

CREATE TABLE IF NOT EXISTS emp_external (
    id INT,
    emp_name STRING,
    dept_code STRING
)
USING DELTA
LOCATION 'abfss://silver@gen2accountnew.dfs.core.windows.net/external_folder';
==============================================================================================================
------------Structure streaming workflows------------------------
Step 1- Read the streaming data using spark.readStream.format('csv').schema(schema)
from pyspark.sql import SparkSession
sdf = spark.readStream.format("csv").schema(schema).option("header", "true").load("/FileStore/tables/stream_read/")
display(sdf)

Step 2- Write the streaming data using spark.writeStream.format('csv').outputMode('append').option('checkpointLocation').start().awaitTermination()
from pyspark.sql import SparkSession
d_f = sdf.writeStream.format("parquet").outputMode("append").option("path", "/FileStore/tables/stream_write/").option("checkpointLocation", "/FileStore/tables/checkpoint/").start().awaitTermination()

==============================================================================================================
Question- Workflow in databricks 
# Notebook 1: ingest_data

# Use a default path to read the uploaded file
csv_path = "/FileStore/tables/EmpPune.csv"

# Read the CSV file into a DataFrame
df = spark.read.option("header", True).csv(csv_path)

# Show a sample of the data
df.show()

# Save as a table for downstream use
df.write.mode("overwrite").saveAsTable("raw_data")

print("Data ingested and saved as 'raw_data'")


# Notebook 2: transform_data
df = spark.table("raw_data")

# Filtering records where Salary > 80000
filtered_df = df[df['Salary'] > 80000]

filtered_df.write.mode("overwrite").saveAsTable("cleaned_data")

print("Data transformed successfully.")

# Notebook 3: report_data

df = spark.table("cleaned_data")
summary = df.groupBy("Salary").count()

summary.show()
summary.write.mode("overwrite").saveAsTable("summary_report")

print("Summary generated successfully.")





